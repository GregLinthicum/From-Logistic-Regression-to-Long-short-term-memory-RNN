# From-Logistic-Regression-to-Long-short-term-memory-RNN
As the technology emerges from the state where the machine did not even get the words you were uttering and responded in a choppy robotic way to the open space of the creativity unimaginable ever before some basic code snippets never age. 

Taxonomy

Supervised Learning

    Artificial NeuralNetworks ( ANN )
    Multilayer Perceptrons (MLPs)
    Convolutional Neural Networks (CNNs)
    Recurrent Neural Networks (RNNs)
    Long short-term memory (LSTM) 
    
    
Unsupervised Learning

    Self-Organizing Maps (SOM)
    Restricted Boltzman Machines (RBM)
    Deep Boltzman Machines (DBM)
    Deep Believe Networks (DBN)
    Auto-Encoders (AE)
        Sparse Auto-Encoder (SAE)
        Stacked Auto-Encoder (SAE)
        Stacked Sparse Auto-Encoder (SSAE)
        Variational Autoencoder (VAE)





Application

Supervised Learning ( Discriminative )
Typical discriminative models include logistic regression (LR), support vector machines (SVM), conditional random fields (CRFs) (specified over an undirected graph), decision trees, neural networks.
    
    Artificial NeuralNetworks ( ANN )
    Used mainly for Regression and Classification
    
    Multilayer Perceptrons (MLPs)
    
   
    Convolutional Neural Networks (CNNs)
    Used Mainly for Machine Vision
   
    Recurrent Neural Networks (RNNs)
    Usedmainly for Time Series Anaalysis
    
    Long short-term memory (LSTM) 
    
    
Unsupervised Learning ( Generative ) 
Typical generative model approaches include naive Bayes classifiers, Gaussian mixture models, variational autoencoders.
    
    Self-Organizing Maps (SOM)
    Used mainly for Feature Detection.
   
    Restricted Boltzman Machines (RBM)
    Usedmainly for dimensionality reduction, classification, regression, collaborative filtering, feature learning, and topic modeling.
    
    Deep Boltzman Machines (DBM)
    Used Mainly for Recommendation Systems.
    
    Deep Believe Networks (DBN)
    Difference between Deep Belief and Deep Boltzman is that Deep Boltzman is bidirectional on every level.
    
    Auto-Encoders (AE)
    Used Mainly for Recommendation Systems.
    https://medium.com/@venkatakrishna.jonnalagadda/sparse-stacked-and-variational-autoencoder-efe5bfe73b64
    
    Why this code modification? 
    
    Time is money. It is well established in Machine Learning to refer to the error between computed and expected value as a Cost. The error might convert to money in numerous applications that target classification or prediction. On the other hand, the time by which the model renders its verdict is not negligible from a cost perspective.
    
    Please have a look at this Java implementation of Neural Network by Deus Jeraldy with a timestamp added and see yourself that the same cost ( as defined by the model) can be reached within substantially different times in a function of the number oh hidden nodes, activation function used, or a number of predefined iterations.
    

