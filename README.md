# From-Logistic-Regression-to-Long-short-term-memory-RNN
As the technology emerges from the state where the machine did not even get the words you were uttering and responded in a choppy robotic way to the open space of the creativity unimaginable ever before some basic code snippets never age. 

Natural Language Processing (NLP) advanced to another milestone by abandonig the long short-term memory (LSTM) RNNs and moving towards Transformers - such as Google's [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) [project](https://github.com/google-research/bert) or Amazon [Research's](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) [SageMaker](https://aws.amazon.com/blogs/machine-learning/fine-tuning-a-pytorch-bert-model-and-deploying-it-with-amazon-elastic-inference-on-amazon-sagemaker/) BERT implementation .


What is this code snipet though?

Uploaded as Java 8 (javaSE-1.8) project under Eclipse Version: 2020-09 (4.17.0) Build id: 20200910-1200
 
Time is money. It is well established in Machine Learning to refer to the error between computed and expected value as a Cost. 
The error might convert to money in numerous applications that target classification or prediction. 
On the other hand, the time by which the model renders its verdict is not negligible from a cost perspective as well.
  
Please download Eclipse project "NeuralNetworks", from the folder above. It is a Java implementation of Neural Network by [Jeraldy Deus](https://medium.com/coinmonks/implementing-an-artificial-neural-network-in-pure-java-no-external-dependencies-975749a38114) with a timestamp added 
and see yourself - by changing the number oh hidden nodes, activation function used, or a number of predefined iterations -
that the same [Cost](https://en.wikipedia.org/wiki/Loss_function) (as defined by the ML/NN) [.](http://reed.cs.depaul.edu/peterh/class/csc380/sessions/week10.html?print-pdf) [.](https://www.cs.utexas.edu/~bornholt/post/synthesis-explained.html) [.](http://alumni.soe.ucsc.edu/~pkaragia/snippets.html) can be reached within substantially different computing times. 
   
    
[IA Wiki in making](https://github.com/GregLinthicum/From-Logistic-Regression-to-Long-short-term-memory-RNN/wiki)

[Benefits of Noise](https://github.com/GregLinthicum/From-Logistic-Regression-to-Long-short-term-memory-RNN/wiki/Noise-increases-precision!--A-proven-engineering-principle.---EXPLAINED#adding-the-noise-solves-serious-technical-limitation-a-low-precision)

[Screnshot Blank](https://github.com/GregLinthicum/From-Logistic-Regression-to-Long-short-term-memory-RNN/wiki/Screenshot-security)


[Main](https://github.com/GregLinthicum/From-Logistic-Regression-to-Long-short-term-memory-RNN) 

[Index](https://github.com/GregLinthicum/From-Logistic-Regression-to-Long-short-term-memory-RNN/wiki/Index)

[AI in Penetration testing](https://github.com/GregLinthicum/From-Logistic-Regression-to-Long-short-term-memory-RNN/wiki/AI-in-Security-and-Network-Testing)

[AI in Game Testing](https://github.com/GregLinthicum/From-Logistic-Regression-to-Long-short-term-memory-RNN/wiki/Game-Testing)
