# From-Logistic-Regression-to-Long-short-term-memory-RNN
As the technology emerges from the state where the machine did not even get the words you were uttering and responded in a choppy robotic way to the open space of the creativity unimaginable ever before some basic code snippets never age. 

Natural Language Processing (NLP) advanced to another milestone by abandonig the long short-term memory (LSTM) RNNs and moving towards Transformers - such as Google's [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)) [project](https://github.com/google-research/bert) or Amazon's [research](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) and [SageMaker](https://aws.amazon.com/blogs/machine-learning/fine-tuning-a-pytorch-bert-model-and-deploying-it-with-amazon-elastic-inference-on-amazon-sagemaker/) BERT implementation .


Uploaded as Java 8 (javaSE-1.8) project under Eclipse Version: 2020-09 (4.17.0) Build id: 20200910-1200
 

  Time is money. It is well established in Machine Learning to refer to the error between computed and expected value as a Cost. 
  The error might convert to money in numerous applications that target classification or prediction. 
  On the other hand, the time by which the model renders its verdict is not negligible from a cost perspective as well.
  
  Please have a look at "NeuralNetworks", a Java implementation of Neural Network by Deus Jeraldy with a timestamp added 
  and see yourself - by changing the number oh hidden nodes, activation function used, or a number of predefined iterations -
  that the same Cost (as defined by the ML/NN) can be reached within substantially different computing times. 
   
    
[Wiki](https://github.com/GregLinthicum/From-Logistic-Regression-to-Long-short-term-memory-RNN/wiki)
